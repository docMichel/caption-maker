#!/usr/bin/env python3
"""Module Travel Llama pour enrichissement touristique"""

import logging
import requests
from typing import Dict, Optional

logger = logging.getLogger(__name__)

class TravelEnricher:
    """Enrichissement touristique avec Travel Llama"""
    
    def __init__(self, ollama_client, config, models):
        self.client = ollama_client
        self.config = config
        self.models = models
        self.travel_model = None
        self._tested_models = set()  # Pour √©viter de tester plusieurs fois
    
    def _test_model_availability(self, model_name: str) -> bool:
        """Tester si un mod√®le est vraiment disponible"""
        if model_name in self._tested_models:
            return model_name == self.travel_model
            
        try:
            logger.info(f"üß™ Test de disponibilit√© du mod√®le: {model_name}")
            
            # M√©thode 1: Essayer de lister les mod√®les
            try:
                response = requests.get(f"{self.client.base_url}/api/tags", timeout=5)
                if response.status_code == 200:
                    available_models = [m['name'] for m in response.json().get('models', [])]
                    if model_name in available_models:
                        logger.info(f"‚úÖ {model_name} trouv√© dans la liste des mod√®les")
                        self._tested_models.add(model_name)
                        return True
            except:
                pass
            
            # M√©thode 2: Essayer une g√©n√©ration test rapide
            logger.info(f"üîß Test direct du mod√®le {model_name}...")
            test_response = self.client.generate_text(
                model=model_name,
                prompt="Bonjour",
                max_tokens=5
            )
            
            if test_response:
                logger.info(f"‚úÖ {model_name} r√©pond correctement")
                self._tested_models.add(model_name)
                return True
            else:
                logger.warning(f"‚ùå {model_name} ne r√©pond pas")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur test mod√®le {model_name}: {e}")
            return False
    
    def _get_travel_model(self):
        """D√©terminer quel mod√®le utiliser avec test r√©el"""
        if self.travel_model is not None:
            return self.travel_model
            
        # Mod√®les √† tester dans l'ordre
        primary_model = self.models.get('travel_llama', 'llama3.1:70b')
        fallback_model = self.models.get('travel_llama_fallback', 'mistral:7b-instruct')
        
        # Tester le mod√®le principal
        if self._test_model_availability(primary_model):
            self.travel_model = primary_model
            logger.info(f"üåç Travel Llama principal actif: {self.travel_model}")
        else:
            # Tester le fallback
            logger.warning(f"‚ö†Ô∏è {primary_model} non disponible, test du fallback...")
            if self._test_model_availability(fallback_model):
                self.travel_model = fallback_model
                logger.info(f"üîÑ Utilisation du fallback: {self.travel_model}")
            else:
                logger.error("‚ùå Aucun mod√®le Travel disponible!")
                self.travel_model = None
                
        return self.travel_model
    
    async def enrich(self, image_description: str, geo_context: Dict[str, str], 
                     callback=None) -> Optional[str]:
        """Enrichir avec des infos touristiques Travel Llama"""
        try:
            # D√©terminer le mod√®le √† utiliser
            model = self._get_travel_model()
            
            if not model:
                logger.warning("‚ùå Pas de mod√®le Travel disponible")
                if callback:
                    await callback('warning', 'Travel Llama non disponible', 'MODEL_UNAVAILABLE')
                return None
            
            if callback:
                model_info = f"Travel Llama ({model})"
                await callback('progress', 50, f"Enrichissement {model_info}...")
            
            # R√©cup√©rer le prompt depuis la config
            prompt_data = self.config._config.get('travel_enrichment', {})
            
            # Utiliser le prompt principal ou le fallback selon le mod√®le
            if model == self.models.get('travel_llama_fallback'):
                prompt_template = prompt_data.get('fallback_prompt', '')
                params = {'temperature': 0.7, 'max_tokens': 100}
            else:
                prompt_template = prompt_data.get('main_prompt', '')
                params = prompt_data.get('parameters', {})
            
            if not prompt_template:
                logger.warning("‚ùå Pas de prompt Travel Llama dans la config")
                return None
            
            # Formatter le prompt
            formatted_prompt = prompt_template.format(
                location_basic=geo_context.get('location_basic', ''),
                cultural_context=geo_context.get('cultural_context', ''),
                nearby_attractions=geo_context.get('nearby_attractions', ''),
                image_description=image_description
            )
            
            logger.debug(f"üìù Prompt Travel Llama ({len(formatted_prompt)} chars)")
            
            # Appeler le mod√®le
            response = self.client.generate_text(
                model=model,
                prompt=formatted_prompt,
                temperature=params.get('temperature', 0.8),
                max_tokens=params.get('max_tokens', 200)
            )
            
            if response and len(response) > 30:
                if callback:
                    await callback('result', {
                        'step': 'travel_enrichment',
                        'result': {'enrichment': response}
                    })
                    await callback('partial', {
                        'type': 'travel_enrichment',
                        'content': {
                            'text': response,
                            'source': 'travel_llama',
                            'model': model
                        }
                    })
                
                logger.info(f"‚úÖ Travel Llama enrichissement r√©ussi ({len(response)} chars)")
                return response
            else:
                logger.warning(f"‚ö†Ô∏è R√©ponse Travel Llama trop courte ou vide")
                return None
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Travel Llama: {e}")
            import traceback
            traceback.print_exc()
            
            if callback:
                await callback('warning', f"Travel Llama erreur: {str(e)}", 'MODEL_ERROR')
            return None