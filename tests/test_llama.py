#!/usr/bin/env python3
"""
Test de disponibilit√© et fonctionnement de Travel Llama
"""

import requests
import json
import time

def test_ollama_connection(base_url="http://localhost:11434"):
    """Tester la connexion √† Ollama"""
    print(f"üîå Test connexion Ollama sur {base_url}")
    try:
        response = requests.get(f"{base_url}/api/version", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama est accessible")
            return True
        else:
            print(f"‚ùå Ollama retourne code {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Erreur connexion Ollama: {e}")
        return False

def list_available_models(base_url="http://localhost:11434"):
    """Lister tous les mod√®les disponibles"""
    print("\nüìã Liste des mod√®les disponibles:")
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            if models:
                for model in models:
                    name = model.get('name', 'unknown')
                    size = model.get('size', 0) / (1024**3)  # Convert to GB
                    modified = model.get('modified_at', '')[:10]
                    print(f"   - {name} ({size:.1f}GB) - modifi√©: {modified}")
                return [m['name'] for m in models]
            else:
                print("   ‚ö†Ô∏è Aucun mod√®le trouv√©")
                return []
        else:
            print(f"   ‚ùå Erreur: code {response.status_code}")
            return []
    except Exception as e:
        print(f"   ‚ùå Erreur listing: {e}")
        return []

def test_model_generation(model_name, base_url="http://localhost:11434"):
    """Tester la g√©n√©ration avec un mod√®le sp√©cifique"""
    print(f"\nüß™ Test du mod√®le: {model_name}")
    
    prompt = """Tu es Travel Llama, expert en voyage. 
    Enrichis cette description avec des infos touristiques.
    Lieu: Nouvelle-Cal√©donie
    R√©ponds en 2-3 phrases maximum."""
    
    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 100
        }
    }
    
    try:
        print("   ‚è≥ G√©n√©ration en cours...")
        start_time = time.time()
        
        response = requests.post(
            f"{base_url}/api/generate",
            json=payload,
            timeout=60
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            text = result.get('response', '').strip()
            
            if text:
                print(f"   ‚úÖ Succ√®s en {elapsed:.1f}s")
                print(f"   üìù R√©ponse: {text[:200]}...")
                return True
            else:
                print(f"   ‚ùå R√©ponse vide")
                return False
        else:
            print(f"   ‚ùå Erreur HTTP {response.status_code}")
            print(f"   D√©tails: {response.text[:200]}")
            return False
            
    except requests.Timeout:
        print(f"   ‚è±Ô∏è Timeout apr√®s 60s")
        return False
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False

def pull_model(model_name, base_url="http://localhost:11434"):
    """T√©l√©charger un mod√®le s'il n'est pas disponible"""
    print(f"\nüì• T√©l√©chargement du mod√®le: {model_name}")
    print("   ‚ö†Ô∏è Cela peut prendre plusieurs minutes...")
    
    payload = {
        "name": model_name,
        "stream": True
    }
    
    try:
        response = requests.post(
            f"{base_url}/api/pull",
            json=payload,
            stream=True,
            timeout=None  # Pas de timeout pour le t√©l√©chargement
        )
        
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line)
                    status = data.get('status', '')
                    
                    if 'pulling' in status:
                        completed = data.get('completed', 0) / (1024**3)
                        total = data.get('total', 0) / (1024**3)
                        if total > 0:
                            percent = (completed / total) * 100
                            print(f"   üìä Progression: {percent:.1f}% ({completed:.1f}/{total:.1f} GB)", end='\r')
                    elif 'success' in status.lower():
                        print(f"\n   ‚úÖ {status}")
                    else:
                        print(f"   ‚ÑπÔ∏è {status}")
                        
                except json.JSONDecodeError:
                    continue
                    
        print("\n   ‚úÖ T√©l√©chargement termin√©")
        return True
        
    except Exception as e:
        print(f"\n   ‚ùå Erreur t√©l√©chargement: {e}")
        return False

def main():
    """Test principal"""
    print("üåç TEST TRAVEL LLAMA")
    print("=" * 50)
    
    base_url = "http://localhost:11434"
    
    # 1. Tester la connexion
    if not test_ollama_connection(base_url):
        print("\n‚ùå Ollama n'est pas accessible. V√©rifiez qu'il est lanc√©.")
        return
    
    # 2. Lister les mod√®les
    available_models = list_available_models(base_url)
    
    # 3. Mod√®les √† tester
    travel_models = [
        "llama3.1:70b",      # Mod√®le principal
        "llama3.1:8b",       # Version plus l√©g√®re
        "llama3.1:latest",   # Version par d√©faut
        "mistral:7b-instruct" # Fallback
    ]
    
    print("\nüîç Recherche des mod√®les Travel Llama...")
    found_models = []
    
    for model in travel_models:
        if model in available_models:
            print(f"   ‚úÖ {model} est install√©")
            found_models.append(model)
        else:
            print(f"   ‚ùå {model} n'est pas install√©")
    
    # 4. Tester les mod√®les trouv√©s
    if found_models:
        print("\nüß™ Test des mod√®les trouv√©s:")
        working_models = []
        
        for model in found_models:
            if test_model_generation(model, base_url):
                working_models.append(model)
        
        if working_models:
            print(f"\n‚úÖ Mod√®les fonctionnels: {', '.join(working_models)}")
            print(f"üí° Recommandation: utilisez '{working_models[0]}' comme mod√®le principal")
        else:
            print("\n‚ùå Aucun mod√®le ne fonctionne correctement")
    else:
        print("\n‚ö†Ô∏è Aucun mod√®le Travel Llama trouv√©")
        print("\nüí° Pour installer llama3.1, ex√©cutez:")
        print("   ollama pull llama3.1:8b    # Version l√©g√®re (4.9GB)")
        print("   ollama pull llama3.2:3b   # Version compl√®te (40GB)")
        
        # Proposer le t√©l√©chargement
        response = input("\nVoulez-vous t√©l√©charger llama3.1:8b maintenant? (o/n): ")
        if response.lower() == 'o':
            if pull_model("llama3.1:8b", base_url):
                print("\nüéâ Installation r√©ussie! Relancez ce test.")
            else:
                print("\n‚ùå √âchec de l'installation")

if __name__ == "__main__":
    main()